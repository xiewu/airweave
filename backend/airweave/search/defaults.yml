search_defaults:
  offset: 0
  limit: 100

  retrieval_strategy: hybrid

  # Temporal relevance is disabled while Vespa support is pending
  # Set to 0 or false to disable; set to a float (0.0-1.0) to enable
  temporal_relevance: false

  expand_query: true
  interpret_filters: false
  rerank: true
  generate_answer: true

# Model specifications per provider
# Defines available models and their properties
provider_models:
  openai:
    llm_big:
      name: "gpt-5"
      tokenizer: "cl100k_base"
      context_window: 400000
    llm_small:
      name: "gpt-5-nano"
      tokenizer: "cl100k_base"
      context_window: 400000
    embedding_large:
      name: "text-embedding-3-large"
      tokenizer: "cl100k_base"
      dimensions: 3072
      max_tokens: 8192
    embedding_small:
      name: "text-embedding-3-small"
      tokenizer: "cl100k_base"
      dimensions: 1536
      max_tokens: 8192
    embedding:
      name: "text-embedding-3-large"
      tokenizer: "cl100k_base"
      dimensions: 3072
      max_tokens: 8192
    rerank:
      name: "gpt-5-nano"
      tokenizer: "cl100k_base"
      context_window: 400000

  groq:
    llm_big:
      name: "openai/gpt-oss-120b"
      tokenizer: "o200k_harmony"
      context_window: 128000
    llm_small:
      name: "openai/gpt-oss-20b"
      tokenizer: "o200k_harmony"
      context_window: 128000
    embedding: null
    rerank:
      name: "openai/gpt-oss-120b"
      tokenizer: "o200k_harmony"
      context_window: 128000

  cerebras:
    llm:
      name: "gpt-oss-120b"
      tokenizer: "o200k_harmony"
      context_window: 131000
    embedding: null
    rerank: null

  cohere:
    llm: null
    embedding: null
    rerank:
      name: "rerank-v3.5"
      tokenizer: "cl100k_base"
      max_tokens_per_doc: 8192
      max_documents: 1000

# Operation preferences - which provider and which models to use
# Format: provider: {llm: model_key, embedding: model_key, rerank: model_key}
operation_preferences:
  query_expansion:
    order:
      - provider: cerebras
        llm: llm
        embedding: null
        rerank: null
      - provider: groq
        llm: llm_big
        embedding: null
        rerank: null
      - provider: openai
        llm: llm_small
        embedding: null
        rerank: null

  query_interpretation:
    order:
      - provider: cerebras
        llm: llm
        embedding: null
        rerank: null
      - provider: groq
        llm: llm_big
        embedding: null
        rerank: null
      - provider: openai
        llm: llm_small
        embedding: null
        rerank: null

  embed_query:
    order:
      - provider: openai
        llm: null
        embedding: embedding
        rerank: null

  reranking:
    order:
      - provider: cohere
        llm: null
        embedding: null
        rerank: rerank
      - provider: groq
        llm: llm_small
        embedding: null
        rerank: rerank
      - provider: openai
        llm: llm_small
        embedding: null
        rerank: rerank

  generate_answer:
    order:
      - provider: cerebras
        llm: llm
        embedding: null
        rerank: null
      - provider: groq
        llm: llm_big
        embedding: null
        rerank: null
      - provider: openai
        llm: llm_small
        embedding: null
        rerank: null

  federated_search:
    order:
      - provider: cerebras
        llm: llm
        embedding: null
        rerank: null
      - provider: groq
        llm: llm_big
        embedding: null
        rerank: null
      - provider: openai
        llm: llm_small
        embedding: null
        rerank: null
