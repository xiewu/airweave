"""Agentic search planner.

The planner builds search plans by analyzing the user query, collection metadata,
and search history. It uses an LLM with structured output to generate plans.
"""

from pathlib import Path

from airweave.core.logging import ContextualLogger
from airweave.search.agentic_search.external.llm.interface import AgenticSearchLLMInterface
from airweave.search.agentic_search.external.tokenizer.interface import (
    AgenticSearchTokenizerInterface,
)
from airweave.search.agentic_search.schemas.filter import format_filter_groups_md
from airweave.search.agentic_search.schemas.history import AgenticSearchHistory
from airweave.search.agentic_search.schemas.plan import AgenticSearchPlan
from airweave.search.agentic_search.schemas.state import AgenticSearchState


class AgenticSearchPlanner:
    """Plans search queries based on state and history.

    The planner:
    1. Builds a prompt from static context, collection metadata, user query, and history
    2. Manages token budget to fit history within context window
    3. Calls the LLM to generate a structured search plan

    History now contains plan + result_brief + evaluation (compact, fits more iterations).
    """

    # Path to context markdown files (relative to this module)
    CONTEXT_DIR = Path(__file__).parent.parent / "context"

    # Reserve this fraction of context_window for reasoning (CoT)
    REASONING_BUFFER_FRACTION = 0.15

    def __init__(
        self,
        llm: AgenticSearchLLMInterface,
        tokenizer: AgenticSearchTokenizerInterface,
        logger: ContextualLogger,
    ) -> None:
        """Initialize the planner.

        Args:
            llm: LLM interface for generating plans.
            tokenizer: Tokenizer for counting tokens.
            logger: Logger for debugging.
        """
        self._llm = llm
        self._tokenizer = tokenizer
        self._logger = logger

        # Track how many history iterations fit (set during prompt building)
        self._history_shown: int = 0
        self._history_total: int = 0

        # Load static context files once
        self._airweave_overview = self._load_context_file("airweave_overview.md")
        self._planner_task = self._load_context_file("planner_task.md")

    def _load_context_file(self, filename: str) -> str:
        """Load a context markdown file.

        Args:
            filename: Name of the file in the context directory.

        Returns:
            Contents of the file.

        Raises:
            FileNotFoundError: If the file doesn't exist.
        """
        filepath = self.CONTEXT_DIR / filename
        return filepath.read_text()

    @property
    def history_shown(self) -> int:
        """Number of detailed history iterations included in the planner prompt."""
        return self._history_shown

    @property
    def history_total(self) -> int:
        """Total number of past iterations."""
        return self._history_total

    async def plan(self, state: AgenticSearchState) -> AgenticSearchPlan:
        """Generate a search plan based on the current state.

        Args:
            state: The current agentic_search state.

        Returns:
            A search plan generated by the LLM.
        """
        system_prompt = self._build_system_prompt()
        user_prompt = self._build_user_prompt(state)

        system_tokens = self._tokenizer.count_tokens(system_prompt)
        user_tokens = self._tokenizer.count_tokens(user_prompt)
        self._logger.debug(
            f"[Planner] Prompt tokens: system={system_tokens:,}, user={user_tokens:,}, "
            f"total={system_tokens + user_tokens:,}"
        )
        self._logger.debug(
            f"[Planner] === SYSTEM PROMPT ===\n{system_prompt}\n"
            f"[Planner] === USER PROMPT ===\n{user_prompt}\n"
            f"[Planner] === END PROMPTS ==="
        )

        plan = await self._llm.structured_output(
            user_prompt, AgenticSearchPlan, system_prompt=system_prompt
        )

        # Log the generated plan
        self._logger.debug(f"[Planner] Generated plan:\n{plan.to_md()}")

        return plan

    def _build_system_prompt(self) -> str:
        """Build the system prompt (static instructions).

        Contains the airweave overview and planner task description.
        These are the rules and guidelines the model should follow.

        Returns:
            The system prompt string.
        """
        return f"""# Airweave Overview

{self._airweave_overview}

---

{self._planner_task}"""

    def _build_user_prompt(self, state: AgenticSearchState) -> str:
        """Build the user prompt (dynamic context + history).

        Contains collection metadata, user query, iteration number, and search history.
        Token budget is managed to fit history within the context window.

        Args:
            state: The current agentic_search state.

        Returns:
            The user prompt string.
        """
        # Build the static context part
        context_prompt = f"""## Context for This Search

### User Request

User query: {state.user_query}
User filter: {format_filter_groups_md(state.user_filter)}
Mode: {state.mode.value}

### Collection Information

{state.collection_metadata.to_md()}

### Current Iteration

This is iteration **{state.iteration_number}**."""

        context_tokens = self._tokenizer.count_tokens(context_prompt)

        # Calculate token budget for history (accounting for system prompt too)
        system_tokens = self._tokenizer.count_tokens(self._build_system_prompt())
        budget_for_history = self._calculate_history_budget(system_tokens + context_tokens)

        # Build history section with budget
        history_info = AgenticSearchHistory.render_md(
            state.history, self._tokenizer, budget_for_history
        )
        self._history_shown = history_info.iterations_shown
        self._history_total = history_info.iterations_total

        # Add consolidation instructions if in consolidation mode
        consolidation_section = ""
        if state.is_consolidation:
            consolidation_section = """

---

## CONSOLIDATION PASS

**The search loop has ended without finding a direct answer.** Your job is now different:
generate ONE search that will surface the **most relevant results** found across all
iterations for the composer to work with.

Look at the history â€” which iterations found partially relevant content?
Which sources/types/parents/documentswere promising but only partially explored?
Generate a search that targets that content.

**You may use any filter level** without progressive
restrictions. The hierarchy rules are suspended for this pass. The goal is to give the
user still the best possible material there is even if there is no direct answer.
"""

        return f"""{context_prompt}
{consolidation_section}
---

## Search History

{history_info.markdown}
"""

    def _calculate_history_budget(self, static_tokens: int) -> int:
        """Calculate how many tokens are available for history.

        Token budget breakdown:
        - context_window = input + reasoning + output
        - Reserve max_output_tokens for output
        - Reserve a fraction for reasoning (CoT)
        - Reserve space for truncation notices (worst case)
        - The rest is available for input (static prompt + history)

        Args:
            static_tokens: Tokens used by the static prompt.

        Returns:
            Number of tokens available for history content.
        """
        model_spec = self._llm.model_spec

        # Total available for input = context_window - output - reasoning_buffer
        reasoning_buffer = int(model_spec.context_window * self.REASONING_BUFFER_FRACTION)
        max_input_tokens = (
            model_spec.context_window - model_spec.max_output_tokens - reasoning_buffer
        )

        # Reserve space for truncation notices
        truncation_reserve = AgenticSearchHistory.get_truncation_reserve_tokens(self._tokenizer)

        # Budget for history = input budget - static prompt - truncation reserve
        budget = max_input_tokens - static_tokens - truncation_reserve

        # Ensure non-negative
        return max(0, budget)
