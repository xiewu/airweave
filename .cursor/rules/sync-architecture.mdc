---
globs: **/sync/**
alwaysApply: false
---
# Airweave Sync Architecture - Deep Dive

## Overview

The sync module orchestrates data flow from sources to destinations using a highly concurrent, pull-based asynchronous architecture with sophisticated backpressure control, real-time progress tracking, and automatic OAuth token management.

## Core Architecture Principles

### 1. Pull-Based Concurrency Model
- **Worker Pool Pattern**: Uses `AsyncWorkerPool` with semaphore-controlled concurrency (default: 20 workers)
- **Pull vs Push**: Workers pull entities from the stream only when ready, preventing system overload
- **Backpressure**: `AsyncSourceStream` uses bounded queues (default: 10000) to naturally throttle producers

### 2. Separation of Concerns
- **Producer/Consumer Decoupling**: Source generation runs independently from entity processing
- **Modular Pipeline**: Each stage (enrich, transform, vectorize, persist) is isolated
- **Resource Isolation**: Database sessions created only when needed to minimize connection usage

## Component Deep Dive

### SyncFactory
**Purpose**: Factory that builds SyncContext (data), SyncRuntime (services), and wires them into the orchestrator

**Key Responsibilities**:
- Builds SyncContext (frozen data) via SyncContextBuilder
- Builds live services (source, destinations, trackers) via sub-builders directly
- Assembles SyncRuntime from the services
- Configures contextual logging with sync metadata
- Wires pipelines, handlers, worker pool, and stream

### SyncContext (frozen data)
**Purpose**: Immutable data describing a sync run. Inherits from `BaseContext` (sibling to `ApiContext`).

**Fields** (flat, no sub-contexts):
- `sync_id`, `sync_job_id`, `collection_id`, `source_connection_id`: Scope IDs
- `sync`, `sync_job`, `collection`, `connection`: Schema objects
- `execution_config`, `force_full_sync`, `batch_size`, `max_batch_latency_ms`: Config
- `entity_map`: Maps entity types to UUIDs
- `source_short_name`: Derived from source at build time
- From `BaseContext`: `organization`, `user`, `logger`

Can be passed directly as `ctx` to CRUD operations (it IS a `BaseContext`).

### SyncRuntime (live services)
**Purpose**: Holds live services and mutable state for a sync run. Separate from SyncContext.

**Fields**:
- `source`: Source instance with OAuth token management
- `cursor`: Mutable sync cursor for incremental syncs
- `destinations`: List of destination instances
- `entity_tracker`: Centralized entity state tracker
- `state_publisher`: Publishes progress to Redis pubsub
- `guard_rail`: Rate limiting / usage guard

Built by the factory, held by the orchestrator, injected into pipeline/handler constructors.

### Builders
- **SyncContextBuilder** (`builders/sync.py`): Builds data-only SyncContext
- **SourceContextBuilder** (`builders/source.py`): Builds source + cursor (returns tuple)
- **DestinationsContextBuilder** (`builders/destinations.py`): Builds destinations + entity_map
- **TrackingContextBuilder** (`builders/tracking.py`): Builds entity_tracker, state_publisher, guard_rail

The factory calls sub-builders in parallel where possible, then assembles SyncRuntime.

### SyncOrchestrator
**Purpose**: Coordinates the entire sync workflow with error handling and progress tracking

**Workflow Stages**:
1. **Start**: Updates job status to IN_PROGRESS
2. **Process**: Manages entity streaming and concurrent processing
3. **Complete/Fail**: Updates final status with statistics

**Key Methods**:
- `run()`: Main entry point with try/catch for proper cleanup
- `_process_entities()`: Implements pull-based processing loop
- `_handle_completed_tasks()`: Cleans completed tasks and checks for errors
- `_wait_for_remaining_tasks()`: Ensures all tasks complete before finishing

**Concurrency Management**:
```python
# Workers pull entities only when ready
async for entity in stream.get_entities():
    if entity.airweave_system_metadata.should_skip:
        # Skip without using a worker
        await sync_context.entity_tracker.record_skipped(1)
        continue

    # Submit to worker pool (blocks if all workers busy)
    task = await worker_pool.submit(...)
```

### AsyncSourceStream
**Purpose**: Manages async streaming with backpressure between producer and consumer

**Architecture**:
- **Producer Task**: Runs independently, filling queue from source generator
- **Bounded Queue**: Implements backpressure (blocks producer when full)
- **Consumer Interface**: `get_entities()` yields items as they become available
- **Error Propagation**: Producer exceptions are captured and re-raised to consumer

**Key Features**:
- Context manager support for proper resource cleanup
- Progress logging every 50 items
- Graceful shutdown with timeout handling
- Sentinel value (None) signals end of stream

### EntityPipeline
**Purpose**: Orchestrates entity processing through the action/handler architecture

**Pipeline Stages**:
1. **Track & Dedupe**: EntityTracker records encounter, skips duplicates
2. **Prepare**: Populate fields, enrich metadata, compute content hash
3. **Resolve Actions**: ActionResolver determines INSERT/UPDATE/DELETE/KEEP
4. **Dispatch**: ActionDispatcher routes actions to handlers concurrently

**Action Types** (`platform/sync/actions/types.py`):
- `InsertAction`: New entity, not in database
- `UpdateAction`: Hash changed from stored value
- `DeleteAction`: DeletionEntity from source
- `KeepAction`: Hash matches stored value (no-op)

**Handlers** (`platform/sync/handlers/`):
| Handler | Responsibility | Execution |
|---------|----------------|-----------|
| `VectorDBHandler` | Chunking â†’ embedding â†’ vector DB writes | Concurrent |
| `RawDataHandler` | Captures raw entities to ARF storage | Concurrent |
| `PostgresMetadataHandler` | Persists entity metadata | Sequential (last) |

**Error Handling**:
- Catches exceptions per entity (doesn't fail entire sync)
- Marks failed entities as "skipped" via EntityTracker
- Detailed error logging with entity context

### SyncDAGRouter
**Purpose**: Routes entities through transformation pipeline based on DAG structure

**Key Components**:
- **Execution Route**: Pre-computed routing map for O(1) lookups
- **Transformer Cache**: Pre-loaded transformers to avoid database queries
- **Entity Lineage**: Tracks parent-child relationships through transformations

**Routing Logic**:
```python
# Route map: (producer_node_id, entity_type_id) -> consumer_node_id
route_map[(producer, entity_definition_id)] = consumer_node_id

# Special case: routes to destination return None (stops routing)
if all_edges_go_to_destinations:
    route_map[key] = None
```

**Advanced Features**:
- Handles multi-path routing through DAG
- Optimized for chunk processing (files â†’ chunks)

### AsyncWorkerPool
**Purpose**: Controls concurrent task execution with semaphore-based limiting

**Implementation Details**:
- **Semaphore Control**: Limits active tasks (prevents resource exhaustion)
- **Task Tracking**: Maintains set of pending tasks with cleanup callbacks
- **Detailed Logging**: Tracks task lifecycle (submit â†’ wait â†’ start â†’ complete)
- **Thread Awareness**: Logs thread IDs for debugging concurrency issues

**Key Methods**:
- `submit()`: Creates task and adds to tracking set
- `_run_with_semaphore()`: Wraps coroutine with semaphore acquisition
- `_handle_task_completion()`: Cleans up completed/failed tasks

### EntityTracker
**Purpose**: Centralized entity state tracking (dedup + progress + encounter tracking)

**Responsibilities**:
- **Deduplication**: Tracks `(entity_id, entity_definition_id)` to prevent reprocessing
- **Encounter Counting**: Maintains count of entities by type for stats
- **Progress Stats**: Thread-safe counters for inserted/updated/deleted/kept/skipped

**Key Methods**:
- `track()`: Records entity encounter, returns False if duplicate
- `record_action()`: Increments stat counter (inserted, updated, etc.)
- `get_stats()`: Returns `SyncStats` for job completion
- `get_all_encountered_ids_flat()`: Returns set of all entity IDs (for orphan cleanup)

### SyncStatePublisher
**Purpose**: Publishes entity state to Redis pubsub for real-time updates

**Features**:
- **Threshold Publishing**: Publishes updates every N operations (default: 3)
- **Redis Integration**: Publishes to `sync_job:{job_id}` channels
- **Snapshot Storage**: Stores progress snapshots with TTL for stuck job detection

**Redis Snapshot Storage**:
- Key: `sync_progress_snapshot:{job_id}`
- Includes `last_update_timestamp` for cleanup job to detect stuck syncs
- 30-minute TTL to automatically clean up completed syncs

### TokenManager
**Purpose**: Manages OAuth2 token refresh for long-running syncs

**Key Features**:
- **Automatic Refresh**: Refreshes tokens before expiry (25-minute intervals)
- **Concurrent Refresh Prevention**: Uses async lock to prevent duplicate refreshes
- **Direct Injection**: Supports non-refreshable tokens (skips refresh)

**Refresh Logic**:
```python
# Only refreshes for specific auth types
if auth_type in (AuthType.oauth2_with_refresh, AuthType.oauth2_with_refresh_rotating):
    # Refresh if older than 25 minutes
    if time_since_refresh > REFRESH_INTERVAL_SECONDS:
        await self._refresh_token()
```

### Async Helpers
**Purpose**: Performance utilities for CPU-bound operations

**Key Utilities**:
- **Shared Thread Pool**: Reuses executor for all CPU-bound tasks
- **Async Hashing**: Non-blocking file/content hashing
- **Stable Serialization**: Consistent object serialization for hashing
- **Chunked File Reading**: Memory-efficient file processing

## Data Flow - Detailed

### 1. Initialization Phase
```
SyncFactory.create_orchestrator()
â”œâ”€â”€ Create SyncContext
â”‚   â”œâ”€â”€ Initialize source with OAuth token
â”‚   â”œâ”€â”€ Create destinations
â”‚   â”œâ”€â”€ Setup embedding model
â”‚   â”œâ”€â”€ Build DAG router
â”‚   â”œâ”€â”€ Create EntityTracker
â”‚   â””â”€â”€ Create SyncStatePublisher
â”œâ”€â”€ Initialize transformer cache (CRITICAL)
â”œâ”€â”€ Create EntityPipeline
â”‚   â”œâ”€â”€ ActionResolver
â”‚   â””â”€â”€ ActionDispatcher (with handlers)
â””â”€â”€ Create AsyncWorkerPool
```

### 2. Streaming Phase
```
AsyncSourceStream (Producer)
â”œâ”€â”€ Runs in separate task
â”œâ”€â”€ Generates entities from source
â”œâ”€â”€ Puts in bounded queue (backpressure)
â””â”€â”€ Signals completion with None

SyncOrchestrator (Consumer)
â”œâ”€â”€ Pulls from stream when worker available
â”œâ”€â”€ Skips marked entities without worker
â””â”€â”€ Submits to worker pool
```

### 3. Processing Phase (Per Batch)
```
EntityPipeline.process()
â”œâ”€â”€ Track & dedupe (EntityTracker)
â”œâ”€â”€ Prepare entities (enrich, compute hash)
â”œâ”€â”€ Resolve actions (ActionResolver)
â”‚   â”œâ”€â”€ Query DB for existing hashes
â”‚   â””â”€â”€ Return ActionBatch (inserts, updates, deletes, keeps)
â””â”€â”€ Dispatch to handlers (ActionDispatcher)
    â”œâ”€â”€ VectorDBHandler (concurrent)
    â”‚   â””â”€â”€ text â†’ chunks â†’ embeddings â†’ vector DB
    â”œâ”€â”€ RawDataHandler (concurrent)
    â”‚   â””â”€â”€ Capture to ARF storage
    â””â”€â”€ PostgresMetadataHandler (sequential, last)
        â””â”€â”€ Persist entity metadata
```

### 4. Progress Tracking
```
EntityTracker records action
â”œâ”€â”€ Async lock ensures thread safety
â”œâ”€â”€ SyncStatePublisher checks threshold (every 3 ops)
â”œâ”€â”€ Publish to Redis pubsub if threshold met
â”‚   â”œâ”€â”€ Real-time updates to subscribers
â”‚   â””â”€â”€ Snapshot stored in Redis (sync_progress_snapshot:{job_id})
â””â”€â”€ Subscribers receive real-time updates
```

## Orphaned Workflow Self-Destruct

**Problem**: Workflows may execute after their sync/source_connection is deleted (race condition between schedule deletion and queued workflows).

**Solution**: Self-healing workflows that detect orphaned state and clean up automatically.

**Detection Points**:
1. **Early** (in `create_sync_job_activity`): Checks if sync exists before creating job
   - Returns `{"_orphaned": True, "sync_id": sync_id}` if not found
2. **Late** (in `run_sync_activity`): Catches `NotFoundException` during execution
   - Raises `Exception("ORPHANED_SYNC: Source connection record not found")`

Custom exception types don't serialize cleanly, so we use a string marker.

**Self-Destruct Flow**:
```python
if sync_job_dict.get("_orphaned") or "ORPHANED_SYNC" in str(e):
    # Call self_destruct_orphaned_sync_activity
    await workflow.execute_activity(
        self_destruct_orphaned_sync_activity,
        args=[sync_dict["id"], ctx_dict, reason],
        start_to_close_timeout=timedelta(minutes=5),
        retry_policy=RetryPolicy(maximum_attempts=3),
    )
    return  # Exit gracefully without error
```

**Cleanup Actions** (in `self_destruct_orphaned_sync_activity`):
- Deletes all schedule types: `sync-{id}`, `minute-sync-{id}`, `daily-cleanup-{id}`
- Uses existing `temporal_schedule_service.delete_schedule_handle()`
- Logs with INFO level (ðŸ§¹ indicators), not ERROR
- Idempotent (safe for concurrent workflows)

**Result**: No "Source connection record not found" errors, graceful workflow exits, automatic schedule cleanup.


## Best Practices

### When Extending
1. Maintain pull-based architecture
2. Use async locks for shared state
3. Create database sessions sparingly
4. Log with contextual information
5. Handle errors at entity level


### Common Pitfalls
1. Don't block the event loop
2. Avoid unbounded concurrency
3. Handle token refresh properly
4. Clean up resources in finally blocks
5. Test with large datasets

This architecture enables efficient processing of millions of entities with optimal resource usage, real-time monitoring, and robust error handling.
